{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexanderVIDALRossRINGJARVI_EE598PROJECT",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR-S58fkmqoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "4ed69b95-b10f-4f8c-c55c-36f3a3d881ac"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as LA\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "from scipy.special import softmax\n",
        "from autograd import grad\n",
        "import time\n",
        "\n",
        "X, y = loadlocal_mnist(\n",
        "        images_path='/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/train-images-idx3-ubyte',\n",
        "        labels_path='/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/train-labels-idx1-ubyte')\n",
        "\n",
        "X_test, y_test = loadlocal_mnist(\n",
        "        images_path='/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/t10k-images-idx3-ubyte',\n",
        "        labels_path='/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/t10k-labels-idx1-ubyte')\n",
        "\n",
        "\n",
        "X = X[0:60000,:]\n",
        "X = X/255\n",
        "y = y[0:60000,]\n",
        "y = np.equal.outer(y,np.unique(y)).astype(int)\n",
        "\n",
        "\n",
        "X_test = X_test[0:1000,:]\n",
        "X_test = X_test/255\n",
        "y_test = y_test[0:1000,]\n",
        "y_test = np.equal.outer(y_test,np.unique(y_test)).astype(int)\n",
        "'''\n",
        "def softmax(x):\n",
        "    ps = np.empty(x.shape)\n",
        "    for i in range(x.shape[0]):\n",
        "        ps[i,:] = np.exp(x[i,:] - np.max(x[i,:]))\n",
        "        ps[i,:]/= np.sum(ps[i,:])\n",
        "    return ps\n",
        "'''\n",
        "def h(X,w):\n",
        "    #The logistic function\n",
        "    #X : Data\n",
        "    #w : Weightsdef softmax(x):\n",
        "    return softmax(X@w,axis=1)\n",
        "\n",
        "def cross_entropy(X, w, y):\n",
        "    #The Cross Entropy Function\n",
        "    #X : Data\n",
        "    #w : Weights\n",
        "    #y : labels\n",
        "    #lam : regularization coefficient for Tikonoff Regularization\n",
        "    eps = 1e-18\n",
        "    classes = 10\n",
        "    a = h(X, w)\n",
        "    return (1/classes)*np.sum(-(y*np.log(a+eps)))\n",
        "\n",
        "def cross_entropy_grad(X,w,y):\n",
        "    #First uses correct function to zero out any values of 1's that match y one-hot encoded\n",
        "    a=h(X,w)\n",
        "    grad = X.T@(a - y)\n",
        "    return grad\n",
        "\n",
        "def backtracking(xk, dk, feval, grad, alpha0, rho, c):\n",
        "    fvk = feval(xk)\n",
        "    gk = grad(xk)\n",
        "    alpha = alpha0\n",
        "    while feval(xk + alpha*dk) > fvk + c*alpha*LA.norm(gk.T@dk):\n",
        "        alpha = rho*alpha\n",
        "    return alpha\n",
        "\n",
        "def eval(X,w,y):\n",
        "\n",
        "    a=np.argmax(softmax(X@w),axis=1).tolist()\n",
        "    b=np.argmax(y,axis=1).tolist()\n",
        "    total_percent=0\n",
        "    o=0\n",
        "    t=0\n",
        "    th=0\n",
        "    f=0\n",
        "    fi=0\n",
        "    s=0\n",
        "    se=0\n",
        "    e=0\n",
        "    n=0\n",
        "    z=0\n",
        "    for i in range(X.shape[0]):\n",
        "            if a[i]==b[i]:\n",
        "                total_percent+=1\n",
        "                if a[i]==1:\n",
        "                    o+=1\n",
        "                if a[i]==2:\n",
        "                    t+=1\n",
        "                if a[i]==3:\n",
        "                    th+=1\n",
        "                if a[i]==4:\n",
        "                    f+=1\n",
        "                if a[i]==5:\n",
        "                    fi+=1\n",
        "                if a[i]==6:\n",
        "                    s+=1\n",
        "                if a[i]==7:\n",
        "                    se+=1\n",
        "                if a[i]==8:\n",
        "                    e+=1\n",
        "                if a[i]==9:\n",
        "                    n+=1\n",
        "                if a[i]==0:\n",
        "                    z+=1\n",
        "    print(\"Total percent correct=\",(total_percent/X.shape[0])*100)\n",
        "    print(\"Percent of 0's correct=\", z/b.count(0)*100)\n",
        "    print(\"Percent of 1's correct=\", o/b.count(1)*100)\n",
        "    print(\"Percent of 2's correct=\", t/b.count(2)*100)\n",
        "    print(\"Percent of 3's correct=\", th/b.count(3)*100)\n",
        "    print(\"Percent of 4's correct=\", f/b.count(4)*100)\n",
        "    print(\"Percent of 5's correct=\", fi/b.count(5)*100)\n",
        "    print(\"Percent of 6's correct=\", s/b.count(6)*100)\n",
        "    print(\"Percent of 7's correct=\", se/b.count(7)*100)\n",
        "    print(\"Percent of 8's correct=\", e/b.count(8)*100)\n",
        "    print(\"Percent of 9's correct=\", n/b.count(9)*100)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-875309e348ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m X, y = loadlocal_mnist(\n\u001b[1;32m     10\u001b[0m         \u001b[0mimages_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/train-images-idx3-ubyte'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         labels_path='/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/train-labels-idx1-ubyte')\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m X_test, y_test = loadlocal_mnist(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mlxtend/data/local_mnist.py\u001b[0m in \u001b[0;36mloadlocal_mnist\u001b[0;34m(images_path, labels_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlbpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         magic, n = struct.unpack('>II',\n\u001b[1;32m     38\u001b[0m                                  lbpath.read(8))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/train-labels-idx1-ubyte'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb7EPAF4nApQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as LA\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "from scipy.special import softmax\n",
        "from autograd import grad\n",
        "import time\n",
        "\n",
        "def steepest_descent(w0, feval, grad, stepsize, options = {'gtol':1e-6,'MaxIter':1e6,'disp':True, 'disp_interval':100}):\n",
        "\n",
        "    if 'gtol' in options:\n",
        "        gtol = options['gtol']\n",
        "    else:\n",
        "        gtol = 1e-6\n",
        "\n",
        "    if 'disp' in options:\n",
        "        disp = options['disp']\n",
        "    else:\n",
        "        disp = True\n",
        "\n",
        "    if 'MaxIter' in options:\n",
        "        MaxIter = options['MaxIter']\n",
        "    else:\n",
        "        MaxIter = 1e3\n",
        "\n",
        "    if 'disp_interval' in options:\n",
        "        disp_interval = options['disp_interval']\n",
        "    else:\n",
        "        disp_interval = 100\n",
        "    w = w0\n",
        "    fv = feval(w)\n",
        "    g = grad(w)\n",
        "    ng = LA.norm(g)\n",
        "    history = []\n",
        "    iter_count = 0\n",
        "    if disp:\n",
        "        print('{:15s}{:20s}{:20s}{:15s}'.format('Iteration #', 'Function Value', 'Gradient Norm w', 'Step Size'))\n",
        "    while ng > gtol and iter_count <= MaxIter: # Use the gradient as a stopping criteria\n",
        "        alpha = stepsize(w, g, iter_count)\n",
        "        w = w - alpha*g\n",
        "        # Record the history\n",
        "        fv = feval(w)\n",
        "        g = grad(w)\n",
        "        ng = LA.norm(g)\n",
        "        history.append((fv, ng))\n",
        "        iter_count += 1\n",
        "        if disp and iter_count % disp_interval == 0:\n",
        "            print('{:<15d}{:<20.1f}{:<20.8f}{:<15.6f}'.format(iter_count, fv, ng, alpha))\n",
        "\n",
        "    return w, fv, g, history, iter_count\n",
        "\n",
        "tic = time.time()\n",
        "w0 = np.random.normal(0,.01, (X.shape[1], y.shape[1]))\n",
        "\n",
        "eps = 0\n",
        "MaxIter = 10000\n",
        "disp_interval = 10\n",
        "options = {'gtol':eps, 'MaxIter':MaxIter, 'disp_interval':disp_interval}\n",
        "\n",
        "lam = 0\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y)\n",
        "\n",
        "stepsize = lambda wk, dk, i: 1e-4\n",
        "w, fv, g, history, iter_count1 = steepest_descent(w0, feval, grad, stepsize, options)\n",
        "\n",
        "history_fv1 = [item[0] for item in history]\n",
        "history_ng1 = [item[1] for item in history]\n",
        "\n",
        "evaluation = eval(X,w,y)\n",
        "print('The model evaluation is:', evaluation)\n",
        "\n",
        "testeval = eval(X_test,w,y_test)\n",
        "print('The test model evaluation is:', testeval)\n",
        "\n",
        "\n",
        "plt.plot(np.log10(history_ng1))\n",
        "plt.title('Steepest Descent')\n",
        "plt.xlabel('Iteration $k$', fontsize = 20)\n",
        "plt.ylabel(r'$\\log_{10}\\|\\nabla f(x_k)\\|_2^2$', fontsize = 20)\n",
        "plt.show()\n",
        "toc = time.time()\n",
        "\n",
        "timing = (toc-tic)/MaxIter\n",
        "print('Iteration time', timing)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARph1KxUm1ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as LA\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "from scipy.special import softmax\n",
        "from autograd import grad\n",
        "import time\n",
        "\n",
        "def SGD(X, w0, y, batch_size, feval, grad, stepsize, options = {'gtol':1e-6,'MaxIter':1e3,'disp':True, 'disp_interval':100}):\n",
        "\n",
        "    if 'gtol' in options:\n",
        "        gtol = options['gtol']\n",
        "    else:\n",
        "        gtol = 1e-6\n",
        "\n",
        "    if 'disp' in options:\n",
        "        disp = options['disp']\n",
        "    else:\n",
        "        disp = True\n",
        "\n",
        "    if 'MaxIter' in options:\n",
        "        MaxIter = options['MaxIter']\n",
        "    else:\n",
        "        MaxIter = 1e3\n",
        "\n",
        "    if 'disp_interval' in options:\n",
        "        disp_interval = options['disp_interval']\n",
        "    else:\n",
        "        disp_interval = 100\n",
        "    w = w0\n",
        "    a = np.random.randint(0, X.shape[0])\n",
        "    Xs = X[a, :].reshape(1,784)\n",
        "    ys = y[a, :].reshape(1,10)\n",
        "    fv = feval(Xs, w, ys)\n",
        "    g = grad(Xs, w, ys)\n",
        "    ng = np.linalg.norm(g)\n",
        "    history = []\n",
        "    iter_count = 0\n",
        "    if disp:\n",
        "        print('{:15s}{:20s}{:20s}{:15s}'.format('Iteration #', 'Function Value', 'Gradient Norm w', 'Step Size'))\n",
        "    while ng > gtol and iter_count <= MaxIter:\n",
        "        #g = 0\n",
        "        fv = 0\n",
        "        for i in np.arange(0,batch_size):\n",
        "            a = np.random.randint(0, X.shape[0])\n",
        "            Xs = X[a, :].reshape(1,784)\n",
        "            ys = y[a, :].reshape(1,10)\n",
        "            fv += feval(Xs, w, ys)/batch_size\n",
        "            g += grad(Xs, w, ys)/batch_size\n",
        "        alpha = stepsize(w, -g, iter_count)\n",
        "        w = w - alpha*g\n",
        "        ng = np.linalg.norm(g)\n",
        "        history.append((fv, ng))\n",
        "        iter_count += 1\n",
        "        if disp and iter_count % disp_interval == 0:\n",
        "            print('{:<15d}{:<20.1f}{:<20.8f}{:<15.6f}'.format(iter_count, fv, ng, alpha))\n",
        "\n",
        "    return w, fv, g, history, iter_count\n",
        "tic = time.time()\n",
        "\n",
        "w0 = np.random.normal(0,1, (X.shape[1], y.shape[1]))\n",
        "batch_size = 16\n",
        "eps = 1e-6\n",
        "MaxIter = 100000\n",
        "disp_interval = 10\n",
        "options = {'gtol':eps, 'MaxIter':MaxIter, 'disp_interval':disp_interval}\n",
        "\n",
        "feval = lambda X, w, y: cross_entropy(X, w, y)\n",
        "grad = lambda X, w, y: cross_entropy_grad(X, w, y)\n",
        "\n",
        "stepsize = lambda wk, dk, i: 1/(i+1)\n",
        "w, fv, g, history, iter_count1 = SGD(X, w0, y, batch_size, feval, grad, stepsize, options)\n",
        "\n",
        "history_fv1 = [item[0] for item in history]\n",
        "history_ng1 = [item[1] for item in history]\n",
        "\n",
        "evaluation = eval(X,w,y)\n",
        "print('The training model evaluation is:', evaluation)\n",
        "\n",
        "testeval = eval(X_test,w,y_test)\n",
        "print('The test model evaluation is:', testeval)\n",
        "\n",
        "plt.plot(np.log10(history_ng1))\n",
        "plt.title('SGD')\n",
        "plt.xlabel('Iteration $k$', fontsize = 20)\n",
        "plt.ylabel(r'$\\log_{10}\\|\\nabla f(x_k)\\|_2^2$', fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "toc = time.time()\n",
        "\n",
        "timing = (toc-tic)/MaxIter\n",
        "print('Iteration time', timing)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRjj2gQZncdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy.linalg as LA\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "import time\n",
        "\n",
        "X, y = loadlocal_mnist(\n",
        "        images_path='/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/train-images-idx3-ubyte',\n",
        "        labels_path='/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/train-labels-idx1-ubyte')\n",
        "\n",
        "X_test, y_test = loadlocal_mnist(\n",
        "        images_path='/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/t10k-images-idx3-ubyte',\n",
        "        labels_path='/home/alexander/Documents/CSM-Statistics /Semester 2/Numerical Optimization/Project/t10k-labels-idx1-ubyte')\n",
        "\n",
        "size = 60000\n",
        "test_size =10000\n",
        "X = X[0:size,:]\n",
        "X = X/255\n",
        "y = y[0:size,]\n",
        "y = np.equal.outer(y,np.unique(y)).astype(int)\n",
        "\n",
        "y_0 = y[:,0]\n",
        "y_0 = y_0.reshape(size,1)\n",
        "\n",
        "y_1 = y[:,1]\n",
        "y_1 = y_1.reshape(size,1)\n",
        "\n",
        "y_2 = y[:,2]\n",
        "y_2 = y_2.reshape(size,1)\n",
        "\n",
        "y_3 = y[:,3]\n",
        "y_3 = y_3.reshape(size,1)\n",
        "\n",
        "y_4 = y[:,4]\n",
        "y_4 = y_4.reshape(size,1)\n",
        "\n",
        "y_5 = y[:,5]\n",
        "y_5 = y_5.reshape(size,1)\n",
        "\n",
        "y_6 = y[:,6]\n",
        "y_6 = y_6.reshape(size,1)\n",
        "\n",
        "y_7 = y[:,7]\n",
        "y_7 = y_7.reshape(size,1)\n",
        "\n",
        "y_8 = y[:,8]\n",
        "y_8 = y_8.reshape(size,1)\n",
        "\n",
        "y_9 = y[:,9]\n",
        "y_9 = y_9.reshape(size,1)\n",
        "\n",
        "X_test = X_test[0:test_size,:]\n",
        "X_test = X_test/255\n",
        "y_test = y_test[0:test_size,]\n",
        "y_test = np.equal.outer(y_test,np.unique(y_test)).astype(int)\n",
        "\n",
        "def sigmoid(x):\n",
        "    epsl = 1e-18\n",
        "    return 1 / (1 + np.exp(-x + epsl))\n",
        "\n",
        "def h(X,w):\n",
        "    #The logistic function\n",
        "    #X : Data\n",
        "    #w : Weightsdef softmax(x):\n",
        "    return sigmoid(X@w)\n",
        "\n",
        "def cross_entropy(X, w, y):\n",
        "    #The Cross Entropy Function\n",
        "    #X : Data\n",
        "    #w : Weights\n",
        "    #y : labels\n",
        "    #lam : regularization coefficient for Tikonoff Regularization\n",
        "    a = h(X, w)\n",
        "    classes = 2\n",
        "    epsl = 1e-18\n",
        "    return (1/classes)*np.sum(-y*np.log(a + epsl) - (1 - y)*np.log(1 - a + epsl))\n",
        "\n",
        "\n",
        "def cross_entropy_grad(X,w,y):\n",
        "    #First uses correct function to zero out any values of 1's that match y one-hot encoded\n",
        "    a=h(X,w)\n",
        "    grad = X.T@(a - y)\n",
        "    return grad\n",
        "\n",
        "def backtracking(xk, dk, feval, grad, alpha0, rho, c):\n",
        "    fvk = feval(xk)\n",
        "    gk = grad(xk)\n",
        "    alpha = alpha0\n",
        "    while feval(xk + alpha*dk) > fvk + c*alpha*gk.T@dk:\n",
        "        alpha = rho*alpha\n",
        "    return alpha\n",
        "\n",
        "def eval(X,w,y):\n",
        "\n",
        "    a=np.argmax(X@w,axis=1).tolist()\n",
        "    b=np.argmax(y,axis=1).tolist()\n",
        "    total_percent=0\n",
        "    o=0\n",
        "    t=0\n",
        "    th=0\n",
        "    f=0\n",
        "    fi=0\n",
        "    s=0\n",
        "    se=0\n",
        "    e=0\n",
        "    n=0\n",
        "    z=0\n",
        "    for i in range(X.shape[0]):\n",
        "            if a[i]==b[i]:\n",
        "                total_percent+=1\n",
        "                if a[i]==1:\n",
        "                    o+=1\n",
        "                if a[i]==2:\n",
        "                    t+=1\n",
        "                if a[i]==3:\n",
        "                    th+=1\n",
        "                if a[i]==4:\n",
        "                    f+=1\n",
        "                if a[i]==5:\n",
        "                    fi+=1\n",
        "                if a[i]==6:\n",
        "                    s+=1\n",
        "                if a[i]==7:\n",
        "                    se+=1\n",
        "                if a[i]==8:\n",
        "                    e+=1\n",
        "                if a[i]==9:\n",
        "                    n+=1\n",
        "                if a[i]==0:\n",
        "                    z+=1\n",
        "    print(\"Total percent correct=\",(total_percent/X.shape[0])*100)\n",
        "    print(\"Percent of 0's correct=\", z/b.count(0)*100)\n",
        "    print(\"Percent of 1's correct=\", o/b.count(1)*100)\n",
        "    print(\"Percent of 2's correct=\", t/b.count(2)*100)\n",
        "    print(\"Percent of 3's correct=\", th/b.count(3)*100)\n",
        "    print(\"Percent of 4's correct=\", f/b.count(4)*100)\n",
        "    print(\"Percent of 5's correct=\", fi/b.count(5)*100)\n",
        "    print(\"Percent of 6's correct=\", s/b.count(6)*100)\n",
        "    print(\"Percent of 7's correct=\", se/b.count(7)*100)\n",
        "    print(\"Percent of 8's correct=\", e/b.count(8)*100)\n",
        "    print(\"Percent of 9's correct=\", n/b.count(9)*100)\n",
        "\n",
        "def L_BFGS(w0, feval, grad, m, options = {'gtol':1e-6,'MaxIter':1e3,'disp':True, 'disp_interval':100}):\n",
        "\n",
        "    if 'gtol' in options:\n",
        "        gtol = options['gtol']\n",
        "    else:\n",
        "        gtol = 1e-6\n",
        "\n",
        "    if 'disp' in options:\n",
        "        disp = options['disp']\n",
        "    else:\n",
        "        disp = True\n",
        "\n",
        "    if 'MaxIter' in options:\n",
        "        MaxIter = options['MaxIter']\n",
        "    else:\n",
        "        MaxIter = 1e3\n",
        "\n",
        "    if 'disp_interval' in options:\n",
        "        disp_interval = options['disp_interval']\n",
        "    else:\n",
        "        disp_interval = 100\n",
        "    w = w0\n",
        "    fv = feval(w)\n",
        "    g = grad(w)\n",
        "    ng = np.linalg.norm(g)\n",
        "    alpha = backtracking(w, -g, feval, grad, 1, 0.5, 1e-4)\n",
        "    s = -alpha*g\n",
        "    y = grad(w+s) - g\n",
        "    Memory = [[s, y]]\n",
        "    history = []\n",
        "    iter_count = 0\n",
        "\n",
        "    def two_loop(q0, Memory):\n",
        "        q = np.concatenate(q0,axis = 0).ravel(order = 'F')\n",
        "        q = q.reshape(784,1)\n",
        "        m = len(Memory)\n",
        "        epsl = 1e-18\n",
        "        alpha = np.zeros((m,1))\n",
        "        for i, (s, y) in enumerate(reversed(Memory)):\n",
        "            if np.abs(y.T@s) > 1e-8:\n",
        "                rho = 1/(y.T@s + epsl)\n",
        "            else:\n",
        "                rho = 1e5\n",
        "            alpha[-(i+1)] = rho*(s.T@q)\n",
        "            q = q - alpha[-(i+1)]*y\n",
        "        s, y = Memory[-1]\n",
        "        gamma = (s.T@y)/(y.T@y + eps)\n",
        "        r = gamma*q\n",
        "        for i, (s, y) in enumerate(Memory):\n",
        "            if np.abs(y.T@s) > 1e-8:\n",
        "                rho = 1/(y.T@s + epsl)\n",
        "            else:\n",
        "                rho = 1e5\n",
        "            beta = rho*(y.T@r)\n",
        "            r = r + s*(alpha[i]-beta)\n",
        "        return r\n",
        "\n",
        "    if disp:\n",
        "        print('{:15s}{:20s}{:20s}{:15s}'.format('Iteration #', 'Function Value', 'Gradient Norm', 'Step Size'))\n",
        "\n",
        "    while ng > eps and iter_count <= MaxIter:\n",
        "        d = -two_loop(g, Memory)\n",
        "        alpha = backtracking(w, d, feval, grad, 10, 0.5, 1e-4)\n",
        "        w_prev = w\n",
        "        w = w_prev + alpha*d\n",
        "        s = w - w_prev\n",
        "        g_prev = g\n",
        "        g = grad(w)\n",
        "        ng = LA.norm(g)\n",
        "        y = g - g_prev\n",
        "        if len(Memory) >= m:\n",
        "            Memory.pop(0)\n",
        "        Memory.append([s,y])\n",
        "        fv = feval(w)\n",
        "        history.append((fv, ng))\n",
        "        iter_count += 1\n",
        "        if disp and iter_count % disp_interval == 0:\n",
        "            print('{:<15d}{:<20.1f}{:<20.8f}{:<15.6f}'.format(iter_count, fv, ng, alpha))\n",
        "    return w, fv, g, history, iter_count\n",
        "tic = time.time()\n",
        "w0 = np.random.normal(0,.1,(X.shape[1],1))\n",
        "m = 60\n",
        "eps = 5e-1\n",
        "MaxIter = 10\n",
        "disp_interval = 10\n",
        "options = {'gtol':eps, 'MaxIter':MaxIter, 'disp_interval':disp_interval}\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_0)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_0)\n",
        "\n",
        "w_0, fv_0, g_0, history_0, iter_count_0 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_0 = [item[0] for item in history_0]\n",
        "history_ng1_0 = [item[1] for item in history_0]\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_1)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_1)\n",
        "\n",
        "w_1, fv_1, g_1, history_1, iter_count_1 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_1 = [item[0] for item in history_1]\n",
        "history_ng1_1 = [item[1] for item in history_1]\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_2)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_2)\n",
        "\n",
        "w_2, fv_2, g_2, history_2, iter_count_2 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_2 = [item[0] for item in history_2]\n",
        "history_ng1_2 = [item[1] for item in history_2]\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_3)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_3)\n",
        "\n",
        "w_3, fv_3, g_3, history_3, iter_count_3 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_3 = [item[0] for item in history_3]\n",
        "history_ng1_3 = [item[1] for item in history_3]\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_4)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_4)\n",
        "\n",
        "w_4, fv_4, g_4, history_4, iter_count_4 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_4 = [item[0] for item in history_4]\n",
        "history_ng1_4 = [item[1] for item in history_4]\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_5)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_5)\n",
        "\n",
        "w_5, fv_5, g_5, history_5, iter_count_5 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_5 = [item[0] for item in history_5]\n",
        "history_ng1_5 = [item[1] for item in history_5]\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_6)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_6)\n",
        "\n",
        "w_6, fv_6, g_6, history_6, iter_count_6 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_6 = [item[0] for item in history_6]\n",
        "history_ng1_6 = [item[1] for item in history_6]\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_7)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_7)\n",
        "\n",
        "w_7, fv_7, g_7, history_7, iter_count_7 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_7 = [item[0] for item in history_7]\n",
        "history_ng1_7 = [item[1] for item in history_7]\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_8)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_8)\n",
        "\n",
        "w_8, fv_8, g_8, history_8, iter_count_8 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_8 = [item[0] for item in history_8]\n",
        "history_ng1_8 = [item[1] for item in history_8]\n",
        "\n",
        "feval = lambda w: cross_entropy(X, w, y_9)\n",
        "grad = lambda w: cross_entropy_grad(X, w, y_9)\n",
        "\n",
        "w_9, fv_9, g_9, history_9, iter_count_9 = L_BFGS(w0, feval, grad, m, options)\n",
        "\n",
        "history_fv1_9 = [item[0] for item in history_9]\n",
        "history_ng1_9 = [item[1] for item in history_9]\n",
        "\n",
        "w = np.concatenate((w_0, w_1, w_2, w_3, w_4, w_5, w_6, w_7, w_8, w_9)).reshape((-1, 10), order='F')\n",
        "\n",
        "\n",
        "evaluation = eval(X,w,y)\n",
        "print('The training model evaluation is:', evaluation)\n",
        "\n",
        "testeval = eval(X_test,w,y_test)\n",
        "print('The test model evaluation is:', testeval)\n",
        "\n",
        "\n",
        "plt.plot(np.log10(history_ng1_0),label = '0')\n",
        "plt.plot(np.log10(history_ng1_1),label = '1')\n",
        "plt.plot(np.log10(history_ng1_2),label = '2')\n",
        "plt.plot(np.log10(history_ng1_3),label = '3')\n",
        "plt.plot(np.log10(history_ng1_4),label = '4')\n",
        "plt.plot(np.log10(history_ng1_5),label = '5')\n",
        "plt.plot(np.log10(history_ng1_6),label = '6')\n",
        "plt.plot(np.log10(history_ng1_7),label = '7')\n",
        "plt.plot(np.log10(history_ng1_8),label = '8')\n",
        "plt.plot(np.log10(history_ng1_9),label = '9')\n",
        "\n",
        "plt.title('L-BFGS')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.xlabel('Iteration $k$', fontsize = 20)\n",
        "plt.ylabel(r'$\\log_{10}\\|\\nabla f(x_k)|_2^2$', fontsize = 20)\n",
        "plt.show()\n",
        "\n",
        "toc = time.time()\n",
        "\n",
        "timing = (toc-tic)/(MaxIter*10)\n",
        "print('Iteration time', timing)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}